\section{Background}

\subsection{System Model}
Throughout the paper, we assume an asynchronous network model in which messages
can be arbitrarily dropped, delayed, and reordered. We assume machines can fail
by crashing but do not act maliciously; i.e., we do not consider Byzantine
failures. We assume that machines operate at arbitrary speeds, and we do not
assume clock synchronization. Every protocol discussed in this paper assumes
that at most $f$ machines will fail for some configurable $f$.

\subsection{Paxos}

\subsection{Paxos}
\defword{Consensus} is the act of choosing a single value among a set of
proposed values. \defword{Paxos}~\cite{lamport1998part} is the most popular
consensus protocol. We assume the reader is familiar with Paxos, but we pause
to review the parts of the protocol that are most important to understand for
the rest of this paper.

A Paxos deployment that tolerates $f$ faults consists of an arbitrary number of
clients, at least $f+1$ \defword{proposers}, and $2f+1$ \defword{acceptors}, as
illustrated in \figref{PaxosBackgroundDiagram}. When a client wants to propose
a value, it sends the value to a proposer $p$. The proposer then initiates a
two-phase protocol. In Phase 1, the proposer contacts the acceptors and learns
of any values that may have already been chosen. In Phase 2, the proposer
proposes a value to the acceptors, and the acceptors vote to choose it. If a
value receives votes from a majority of the acceptors, the value is considered
chosen.

More concretely, in Phase 1, $p$ sends \msgfont{Phase1a} messages to at least a
majority of the $2f+1$ acceptors. When an acceptor receives a \msgfont{Phase1a}
message, it replies with a \msgfont{Phase1b} message. When the leader receives
\msgfont{Phase1b} messages from a majority of the acceptors, it begins Phase 2.
%
In Phase 2, the proposer sends \msg{Phase2a}{x} messages to the acceptors with
some value $x$. Upon receiving a \msg{Phase2a}{x} message, an acceptor can
either ignore the message, or vote for the value $x$ and return a
\msg{Phase2b}{x} message to the proposer. Upon receiving \msg{Phase2b}{x}
messages from a majority of the acceptors, the proposed value $x$ is considered
chosen.

{\input{figures/paxos_background_diagram.tex}}

For simplicity of exposition, we have skipped over quite a few details. We have
not explained how a proposer chooses a value to propose in Phase 2, we have not
explained how an acceptor decides to vote for a value or ignore it, we did not
discuss rounds, and so on. For the purposes of this paper though, our
simplified discussion should suffice. For a complete description of Paxos,
refer to~\cite{lamport2001paxos}.

\subsection{MultiPaxos}
While consensus is the act of choosing a single value, \defword{state machine
replication} is the act of choosing a sequence (a.k.a.\ log) of values. A state
machine replication protocol manages a number of copies, or \defword{replicas},
of a deterministic state machine. Over time, the protocol constructs a growing
log of state machine commands, and replicas execute the commands in prefix
order.  By beginning in the same initial state, and by executing the same
commands in the same order, all state machine replicas are kept in sync. This
is illustrated in \figref{StateMachineReplicationExample}.

{\input{figures/state_machine_replication_example.tex}}

\defword{MultiPaxos} is one of the simplest and most widely used state machine
replication protocols. Again, we assume the reader is familiar with MultiPaxos,
but we review the most salient bits.
%
MultiPaxos uses one instance of Paxos for every log entry, choosing the
commands in the log one slot at a time.
%
A MultiPaxos deployment that tolerates $f$ faults consists of an arbitrary
number of clients, at least $f+1$ proposers, and $2f+1$ acceptors (like Paxos),
as well as at least $f+1$ replicas, as illustrated in
\figref{MultiPaxosBackgroundDiagram}.

{\input{figures/multipaxos_background_diagram.tex}}

Initially, one of the proposers is elected leader and runs Phase 1 of Paxos for
every single log entry. When a client wants to propose a state machine command
$x$, it sends the command to the leader. The leader assigns the command a log
entry $i$ and then runs Phase 2 of Paxos to get the value chosen in entry $i$.
That is, the leader sends \msg{Phase2a}{i, x} messages to the acceptors to vote
for value $x$ in slot $i$. In the normal case, the acceptors all vote for $x$
in slot $i$ and respond with \msg{Phase2b}{i, x} messages. Once the leader learns
that a command has been chosen in a given log entry (i.e.\ once the leader
receives \msg{Phase2b}{i, x} messages from a majority of the acceptors), it
informs the replicas. Replicas insert chosen commands into their logs and
execute the logs in prefix order.

Note that the leader assigns log entries to commands in increasing order. The
first received command is put in entry $0$, the next command in entry $1$, the
next command in entry $2$, and so on.
%
Also note that even though every replica executes every command, for any given
state machine command $x$, only one replica needs to send the result of
executing $x$ back to the client. For example, log entries can be round-robin
partitioned across the replicas. With $n$ replicas for example, replica $r_i$
returns results for log entries $j$ where $j \bmod n \equiv i$.

\subsection{Compartmentalized MultiPaxos}

compartmentalized paxos introduces three things
- proxy leaders
- multiple acceptor groups
- increased number of replicas

\subsection{Linearizability, Intuitively}

\subsection{Linearizability, Formally}

\subsection{Linearizable Quorum Reads}
