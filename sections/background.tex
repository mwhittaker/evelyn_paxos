\section{Background}

\subsection{System Model}
Throughout the paper, we assume an asynchronous network model in which messages
can be arbitrarily dropped, delayed, and reordered. We assume machines can fail
by crashing but do not act maliciously; i.e., we do not consider Byzantine
failures. We assume that machines operate at arbitrary speeds, and we do not
assume clock synchronization. Every protocol discussed in this paper assumes
that at most $f$ machines will fail for some configurable $f$.

\subsection{Paxos}
\defword{Consensus} is the act of choosing a single value among a set of
proposed values. \defword{Paxos}~\cite{lamport1998part} is the most popular
consensus protocol. We assume the reader is familiar with Paxos, but we pause
to review the parts of the protocol that are most important to understand for
the rest of this paper.

A Paxos deployment that tolerates $f$ faults consists of an arbitrary number of
clients, at least $f+1$ \defword{proposers}, and $2f+1$ \defword{acceptors}, as
illustrated in \figref{PaxosBackgroundDiagram}. When a client wants to propose
a value, it sends the value to a proposer $p$. The proposer then initiates a
two-phase protocol. In Phase 1, the proposer contacts the acceptors and learns
of any values that may have already been chosen. In Phase 2, the proposer
proposes a value to the acceptors, and the acceptors vote to choose it. If a
value receives votes from a majority of the acceptors, the value is considered
chosen.

More concretely, in Phase 1, $p$ sends \msgfont{Phase1a} messages to at least a
majority of the $2f+1$ acceptors. When an acceptor receives a \msgfont{Phase1a}
message, it replies with a \msgfont{Phase1b} message. When the leader receives
\msgfont{Phase1b} messages from a majority of the acceptors, it begins Phase 2.
%
In Phase 2, the proposer sends \msg{Phase2a}{x} messages to the acceptors with
some value $x$. Upon receiving a \msg{Phase2a}{x} message, an acceptor can
either ignore the message, or vote for the value $x$ and return a
\msg{Phase2b}{x} message to the proposer. Upon receiving \msg{Phase2b}{x}
messages from a majority of the acceptors, the proposed value $x$ is considered
chosen.

{\input{figures/paxos_background_diagram.tex}}

For simplicity of exposition, we have skipped over quite a few details. We have
not explained how a proposer chooses a value to propose in Phase 2, we have not
explained how an acceptor decides to vote for a value or ignore it, we did not
discuss rounds, and so on. For the purposes of this paper though, our
simplified discussion should suffice. For a complete description of Paxos,
refer to~\cite{lamport2001paxos}.

\subsection{MultiPaxos}
While consensus is the act of choosing a single value, \defword{state machine
replication} is the act of choosing a sequence (a.k.a.\ log) of values. A state
machine replication protocol manages a number of copies, or \defword{replicas},
of a deterministic state machine. Over time, the protocol constructs a growing
log of state machine commands, and replicas execute the commands in prefix
order.  By beginning in the same initial state, and by executing the same
commands in the same order, all state machine replicas are kept in sync. This
is illustrated in \figref{StateMachineReplicationExample}.

{\input{figures/state_machine_replication_example.tex}}

\defword{MultiPaxos} is one of the simplest and most widely used state machine
replication protocols. Again, we assume the reader is familiar with MultiPaxos,
but we review the most salient bits.
%
MultiPaxos uses one instance of Paxos for every log entry, choosing the
commands in the log one slot at a time.
%
A MultiPaxos deployment that tolerates $f$ faults consists of an arbitrary
number of clients, at least $f+1$ proposers, and $2f+1$ acceptors (like Paxos),
as well as at least $f+1$ replicas, as illustrated in
\figref{MultiPaxosBackgroundDiagram}.

{\input{figures/multipaxos_background_diagram.tex}}

Initially, one of the proposers is elected leader and runs Phase 1 of Paxos for
every single log entry. When a client wants to propose a state machine command
$x$, it sends the command to the leader. The leader assigns the command a log
entry $i$ and then runs Phase 2 of Paxos to get the value chosen in entry $i$.
That is, the leader sends \msg{Phase2a}{i, x} messages to the acceptors to vote
for value $x$ in slot $i$. In the normal case, the acceptors all vote for $x$
in slot $i$ and respond with \msg{Phase2b}{i, x} messages. Once the leader learns
that a command has been chosen in a given log entry (i.e.\ once the leader
receives \msg{Phase2b}{i, x} messages from a majority of the acceptors), it
informs the replicas. Replicas insert chosen commands into their logs and
execute the logs in prefix order.

Note that the leader assigns log entries to commands in increasing order. The
first received command is put in entry $0$, the next command in entry $1$, the
next command in entry $2$, and so on.
%
Also note that even though every replica executes every command, for any given
state machine command $x$, only one replica needs to send the result of
executing $x$ back to the client. For example, log entries can be round-robin
partitioned across the replicas. With $n$ replicas for example, replica $r_i$
returns results for log entries $j$ where $j \bmod n \equiv i$.

\subsection{Compartmentalized MultiPaxos}
Compartmentalized MultiPaxos~\cite{whittaker2020compartmentalized} is a
high-throughput MultiPaxos variant that differs from MultiPaxos in the
following three ways:

\textbf{Difference 1: Proxy Leaders.}
In a MultiPaxos deployment, every state machine command is sent to the leader,
and the leader has to perform disproportionately more work per command than the
acceptors and replicas (in most cases). Because of this, the MultiPaxos leader
is a throughput bottleneck. MultiPaxos can process commands only as fast as the
leader can.

Compartmentalized MultiPaxos alleviates the leader bottleneck by noting that
the MultiPaxos leader performs two independent responsibilities: sequencing and
communication. When the leader receives a state machine command, it assigns the
command a log entry. The first command is assigned log entry 0, the second is
assigned log entry 1, and so on. We call this \emph{sequencing}. The leader
also has to communicate with the acceptors to execute Phase 2 of Paxos. We call
this \emph{communication}. Compartmentalized MultiPaxos decouples the two
responsibilities of sequencing and communication by introducing a set of at
least $f+1$ \defword{proxy leaders}.

After this decoupling, the leader performs sequencing, while the proxy leaders
perform communication. Concretely, when a leader receives a state machine
command, it assigns the command a log entry and then forms a \msgfont{Phase2a}
message, as before. Now, however, it does \emph{not} communicate with the
acceptors.  Instead, it sends the \msgfont{Phase2a} message to a randomly
chosen proxy leader, and the proxy leader performs Phase 2 of Paxos and
notifies the replicas of the chosen command. This is illustrated in
\figref{MultiPaxosProxyLeadersDiagram}.

Note that because the leader load balances commands across the proxy leaders,
and because the proxy leaders are independent, we can scale up the number of
proxy leaders to avoid them being a throughput bottleneck.

{\input{figures/multipaxos_proxy_leaders_diagram.tex}}

\textbf{Difference 2: Acceptor Groups.}
MultiPaxos traditionally deploys a fixed set of $2f+1$ acceptors. For a
\emph{single log entry}, the correctness of MultiPaxos depends crucially on
this fact. However, Compartmentalized MultiPaxos notes that \emph{two
different log entries} are independent and can use two different sets of
acceptors.
%
Thus, Compartmentalized MultiPaxos deploys multiple sets of $2f+1$ acceptors,
called \defword{acceptor groups}, with log entries round-robin partitioned
between them. For example, with two acceptor groups, log entries 0, 2, 4, 6,
$\ldots$ are chosen by the first acceptor group while log entries 1, 3, 5, 7,
$\ldots$ are chosen by the second acceptor group. When a proxy leader receives
a \msgfont{Phase2a} message from the leader in log entry $i$, it is careful to
contact the appropriate acceptor group (i.e.\ acceptor group $i \bmod m$ for
$m$ acceptor groups).
%
This is illustrated in \figref{MultiPaxosMoreAcceptorsDiagram}.

{\input{figures/multipaxos_more_acceptors_diagram.tex}}

\textbf{Difference 3: More Replicas.}
MultiPaxos is typically deployed with $f+1$ state machine replicas. MultiPaxos
\emph{can} be deployed with more replicas, but the MultiPaxos leader has to
contact \emph{every} replica. So, if we increase the number of replicas, the
leader has to send more messages per command and becomes more of a throughput
bottleneck.

Compartmentalized MultiPaxos, on the other hand, can be deployed with a larger
number of replicas without decreasing throughput, as illustrated in
\figref{MultiPaxosMoreReplicasDiagram}. As we increase the number of
Compartmentalized MultiPaxos replicas, every proxy leader has to send more
messages per command, but the leader does not.  Moreover, as we increase the
number of replicas, we can also increase the number of proxy leaders (as
needed) to avoid them becoming a throughput bottleneck.

{\input{figures/multipaxos_more_replicas_diagram.tex}}

\subsection{Linearizability, Intuitively}

\subsection{Linearizability, Formally}

\subsection{Linearizable Quorum Reads}
